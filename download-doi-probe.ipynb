{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import httpx\n",
    "from parsel import Selector\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    # format=\"%(asctime)s | %(levelname)s | %(filename)s:%(lineno)d | %(funcName)s | %(message)s\",\n",
    "    # level=logging.DEBUG,\n",
    "    stream=sys.stdout,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_pdf_link(doi: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extracts a direct PDF link by scraping the final article webpage.\n",
    "\n",
    "    Args:\n",
    "        paper_url: The initial article URL (could be a DOI link).\n",
    "\n",
    "    Returns:\n",
    "        The direct PDF URL if found, otherwise None.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Referer\": \"https://scholar.google.com\",  # Some sites require a referrer\n",
    "    }\n",
    "\n",
    "    # got most of the patterns from here from reverse engineering the unpaywall chrome extension\n",
    "    unpaywall_url = f\"https://api.unpaywall.org/v2/{doi}?email=unpaywall@impactstory.org\"\n",
    "\n",
    "    try:\n",
    "        pdf_url = None\n",
    "\n",
    "        # --- Unpaywall Check ---\n",
    "        async with httpx.AsyncClient(timeout=10) as client:\n",
    "            response = await client.get(unpaywall_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            paper_url = data.get(\"doi_url\")\n",
    "\n",
    "            if data.get(\"is_oa\"):\n",
    "                logger.info(f\"Paper is Open Access according to Unpaywall. DOI: {doi}\")\n",
    "\n",
    "                if data.get(\"best_oa_location\") and data[\"best_oa_location\"].get(\"url_for_pdf\"):\n",
    "                    logger.info(f\"Found direct PDF URL from Unpaywall: {data['best_oa_location']['url_for_pdf']}\")\n",
    "                    pdf_url = data[\"best_oa_location\"][\"url_for_pdf\"]  # Return directly if available\n",
    "                    return pdf_url\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"Paper is NOT Open Access according to Unpaywall. DOI: {doi}\")\n",
    "\n",
    "        # Get final redirected URL (important for DOI links)\n",
    "        async with httpx.AsyncClient(timeout=20, follow_redirects=True) as client:\n",
    "            response = await client.get(paper_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            logger.info(f\"Final URL after redirect: {response.url}\")\n",
    "\n",
    "        final_url = str(response.url)\n",
    "\n",
    "        # async with httpx.AsyncClient(timeout=20, follow_redirects=True) as client:\n",
    "        #     response = await client.get(final_url, headers=headers)\n",
    "        #     response.raise_for_status()\n",
    "\n",
    "        selector = Selector(text=response.text)\n",
    "\n",
    "        # --- Meta Tag Check ---\n",
    "        meta_pdf_url = selector.xpath(\"//meta[@name='citation_pdf_url']/@content\").get()\n",
    "        if meta_pdf_url:\n",
    "            logger.info(f\"Found PDF URL in meta tag: {meta_pdf_url}\")\n",
    "            return meta_pdf_url\n",
    "\n",
    "        # --- Domain-Specific Link Checks ---\n",
    "        for link in selector.xpath(\"//a\"):\n",
    "            href = link.xpath(\"@href\").get()\n",
    "            if not href:\n",
    "                continue\n",
    "\n",
    "            # 1. Nature.com (Pattern 1)\n",
    "            if \"nature.com\" in final_url:\n",
    "                match = re.search(r\"/nature/journal/.+?/pdf/(.+?)\\.pdf$\", href)\n",
    "                if match:\n",
    "                    pdf_url = httpx.URL(final_url).join(href).unicode_string()\n",
    "                    logger.info(f\"Found PDF URL (Nature.com Pattern 1): {pdf_url}\")\n",
    "                    return pdf_url\n",
    "\n",
    "                # 2. Nature.com (Pattern 2)\n",
    "                match = re.search(r\"/articles/nmicrobiol\\d+\\.pdf$\", href)\n",
    "                if match:\n",
    "                    pdf_url = httpx.URL(final_url).join(href).unicode_string()\n",
    "                    logger.info(f\"Found PDF URL (Nature.com Pattern 2): {pdf_url}\")\n",
    "                    return pdf_url\n",
    "\n",
    "            # 3. NEJM\n",
    "            if \"nejm.org\" in final_url:\n",
    "                if link.xpath(\"@data-download-content\").get() == \"Article\":\n",
    "                    pdf_url = httpx.URL(final_url).join(href).unicode_string()\n",
    "                    logger.info(f\"Found PDF URL (NEJM): {pdf_url}\")\n",
    "                    return pdf_url\n",
    "\n",
    "            # 4. Taylor & Francis Online\n",
    "            if \"tandfonline.com\" in final_url:\n",
    "                match = re.search(r\"/doi/pdf/10.+?needAccess=true\", href, re.IGNORECASE)\n",
    "                if match:\n",
    "                    pdf_url = httpx.URL(final_url).join(href).unicode_string()\n",
    "                    logger.info(f\"Found PDF URL (Taylor & Francis): {pdf_url}\")\n",
    "                    return pdf_url\n",
    "\n",
    "            # 5. Centers for Disease Control (CDC)\n",
    "            if \"cdc.gov\" in final_url:\n",
    "                if \"noDecoration\" == link.xpath(\"@class\").get() and re.search(r\"\\.pdf$\", href):\n",
    "                    pdf_url = httpx.URL(final_url).join(href).unicode_string()\n",
    "                    logger.info(f\"Found PDF URL (CDC): {pdf_url}\")\n",
    "                    return pdf_url\n",
    "\n",
    "            # 6. ScienceDirect\n",
    "            if \"sciencedirect.com\" in final_url:\n",
    "                pdf_url_attribute = link.xpath(\"@pdfurl\").get()\n",
    "                if pdf_url_attribute:\n",
    "                    pdf_url = httpx.URL(final_url).join(pdf_url_attribute).unicode_string()\n",
    "                    logger.info(f\"Found PDF URL (ScienceDirect): {pdf_url}\")\n",
    "                    return pdf_url\n",
    "\n",
    "        # 7. IEEE Explore (check within the entire page content)\n",
    "        if \"ieeexplore.ieee.org\" in final_url:\n",
    "            match = re.search(r'\"pdfPath\":\"(.+?)\\.pdf\"', response.text)\n",
    "            if match:\n",
    "                pdf_path = match.group(1) + \".pdf\"\n",
    "                pdf_url = \"https://ieeexplore.ieee.org\" + pdf_path\n",
    "                logger.info(f\"Found PDF URL (IEEE Explore): {pdf_url}\")\n",
    "                return pdf_url\n",
    "\n",
    "        # --- General PDF Pattern Check (Fallback) ---\n",
    "        # use the last 3 characters of the DOI to match the link because it's a commmon pattern\n",
    "        # for it to be included in the URL. This is to avoid false positives.\n",
    "        # Not always the case though.\n",
    "        doi_last_3 = doi[-3:] if len(doi) >= 3 else doi\n",
    "        PDF_PATTERNS = [\n",
    "            \".pdf\",\n",
    "            \"/pdf/\",\n",
    "            \"pdf/\",\n",
    "            \"download\",\n",
    "            \"fulltext\",\n",
    "            \"article\",\n",
    "            \"viewer\",\n",
    "            \"content/pdf\",\n",
    "            \"/nature/journal\",\n",
    "            \"/articles/\",\n",
    "            \"/doi/pdf/\",\n",
    "        ]\n",
    "        pdf_links = selector.css(\"a::attr(href)\").getall()  # get all links here to loop through\n",
    "\n",
    "        for link in pdf_links:  # loop through\n",
    "            if any(pattern in link.lower() for pattern in PDF_PATTERNS):\n",
    "                # check if any of the patterns are in the link and the doi_last_3 is in the link\n",
    "                if doi_last_3 in link.lower():\n",
    "                    pdf_url = httpx.URL(final_url).join(link).unicode_string()\n",
    "                    logger.info(f\"Found PDF link (General Pattern): {pdf_url}\")\n",
    "                    return str(pdf_url)\n",
    "\n",
    "                # if the doi_last_3 is not in the link, check if the link is a pdf, do this as final.\n",
    "                pdf_url = httpx.URL(final_url).join(link).unicode_string()\n",
    "                logger.info(f\"Found PDF link (General Pattern): {pdf_url}\")\n",
    "                return str(pdf_url)\n",
    "\n",
    "        logger.warning(\"No PDF link found on the page.\")\n",
    "        return None\n",
    "\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        logger.error(f\"Unpaywall API error ({e.response.status_code}): {e}\")\n",
    "        if e.response.status_code == 404:\n",
    "            logger.error(f\"Paper with DOI {doi} not found by Unpaywall\")\n",
    "        return None\n",
    "\n",
    "    except httpx.RequestError as e:\n",
    "        logger.error(f\"Request error: {e}\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of when DOI is not openly accessible:\n",
    "\n",
    "https://api.unpaywall.org/v2/10.1109/icetet-sip58143.2023.10151614?email=unpaywall@impactstory.org\n",
    "\n",
    "{\n",
    "\"doi\": \"10.1109/icetet-sip58143.2023.10151614\",\n",
    "\"doi_url\": \"https://doi.org/10.1109/icetet-sip58143.2023.10151614\",\n",
    "\"title\": \"Algorithmic Trading Strategy Using Technical Indicators\",\n",
    "\"genre\": \"proceedings-article\",\n",
    "\"is_paratext\": false,\n",
    "\"published_date\": \"2023-04-28\",\n",
    "\"year\": 2023,\n",
    "\"journal_name\": \"2023 11th International Conference on Emerging Trends in Engineering &amp; Technology - Signal and Information Processing (ICETET - SIP)\",\n",
    "\"is_oa\": false, <-----\n",
    "\"oa_status\": \"closed\",\n",
    "\"has_repository_copy\": false,\n",
    "\"best_oa_location\": null,\n",
    "\"first_oa_location\": null,\n",
    "\"oa_locations\": [],\n",
    "\"oa_locations_embargoed\": [],\n",
    "\"updated\": \"2023-06-20T04:32:38.955606\",\n",
    "}\n",
    "\n",
    "Example of when DOI is openly accessible:\n",
    "\n",
    "https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3516053?email=unpaywall@impactstory.org\n",
    "\n",
    "{\n",
    "\"doi\": \"10.1109/access.2024.3516053\",\n",
    "\"doi_url\": \"https://doi.org/10.1109/access.2024.3516053\",\n",
    "\"title\": \"An Algorithmic Trading Approach Merging Machine Learning with Multi-Indicator Strategies for Optimal Performance\",\n",
    "\"genre\": \"journal-article\",\n",
    "\"is_paratext\": false,\n",
    "\"published_date\": \"2024-01-01\",\n",
    "...,\n",
    "\"is_oa\": true, <-----\n",
    "\"oa_status\": \"gold\",\n",
    "\"has_repository_copy\": false,\n",
    "\"best_oa_location\": {\n",
    "\"url\": \"https://doi.org/10.1109/access.2024.3516053\",\n",
    "\"pmh_id\": null,\n",
    "...,\n",
    "}\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_paper(doi: str, title: str, output_dir: str = \"downloads\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Downloads a paper PDF given its DOI and title.\n",
    "    If Unpaywall fails, it scrapes the article page to find the PDF.\n",
    "\n",
    "    Args:\n",
    "        doi: The DOI of the paper.\n",
    "        title: The title of the paper (for the filename).\n",
    "        output_dir: The directory to save the downloaded PDF.\n",
    "\n",
    "    Returns:\n",
    "        The file path of the downloaded PDF if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Sanitize title for a safe filename\n",
    "        # safe_title = \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in title)\n",
    "        safe_title = re.sub(r\"[^\\w\\-_\\.]\", \"_\", title)\n",
    "        file_name = f\"{safe_title}_{doi.replace('/', '_')}.pdf\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        # Check if file already exists\n",
    "        if os.path.exists(f\"{file_path}\"):\n",
    "            logger.info(f\"Skipping download. PDF for DOI: {doi} already exists at {file_path}.\")\n",
    "            return file_path\n",
    "\n",
    "        pdf_link = await scrape_pdf_link(doi)\n",
    "\n",
    "        if not pdf_link:\n",
    "            logger.error(f\"Could not find a PDF link for DOI: {doi}\")\n",
    "            return None\n",
    "\n",
    "        # Download the PDF\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            try:\n",
    "                headers = {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n",
    "                    \"Accept\": \"application/pdf,text/html,*/*\",\n",
    "                    \"Referer\": \"https://scholar.google.com\",  # Some sites require a referrer\n",
    "                }\n",
    "\n",
    "                response = await client.get(pdf_link, headers=headers, follow_redirects=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "                logger.info(f\"Content-Type received: {content_type}\")\n",
    "\n",
    "                if \"pdf\" not in content_type.lower():\n",
    "                    logger.error(\"The downloaded file is not a PDF!\")\n",
    "                    return None\n",
    "\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "\n",
    "                logger.info(f\"Downloaded PDF for DOI: {doi} to {file_path}\")\n",
    "                return file_path\n",
    "\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                logger.error(f\"HTTP error downloading PDF for DOI {doi}: {e.response.status_code}\")\n",
    "                return None\n",
    "            except httpx.RequestError as e:\n",
    "                logger.error(f\"Request error downloading PDF for DOI {doi}: {e}\")\n",
    "                return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"General error downloading PDF for DOI {doi}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_papers_parallel(results, limit=5, download=False, output_dir=\"downloads\"):\n",
    "    \"\"\"\n",
    "    Displays search results and optionally downloads PDFs in parallel.\n",
    "\n",
    "    Args:\n",
    "        results: The search results dictionary.\n",
    "        limit: The number of top results to display.\n",
    "        download: If True, download PDFs for the displayed results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Search Results (Top {limit}):\")\n",
    "    doi_title_pairs_for_download = []\n",
    "\n",
    "    for item in results.get(\"data\", [])[:limit]:\n",
    "        if download:\n",
    "            if item.get(\"externalIds\") and item[\"externalIds\"].get(\"DOI\"):\n",
    "                doi = item[\"externalIds\"][\"DOI\"]\n",
    "                title = item.get(\"title\", \"Unknown_Title\")\n",
    "                doi_title_pairs_for_download.append((doi, title))  # Collect DOI and title for parallel download\n",
    "                print(f\"      Downloading DOI {doi}\")  # Indicate download is initiated\n",
    "\n",
    "    print()\n",
    "\n",
    "    downloaded_files = []\n",
    "    if download and doi_title_pairs_for_download:\n",
    "        download_tasks = [download_paper(doi, title, output_dir) for doi, title in doi_title_pairs_for_download]\n",
    "        downloaded_files = await asyncio.gather(*download_tasks)\n",
    "\n",
    "        successful_downloads = 0\n",
    "        failed_downloads = 0\n",
    "        for file_path in downloaded_files:\n",
    "            if file_path:\n",
    "                successful_downloads += 1\n",
    "            else:\n",
    "                failed_downloads += 1\n",
    "\n",
    "        logger.info(\"--- Parallel Download Statistics ---\")\n",
    "        logger.info(f\"Total papers attempted: {len(doi_title_pairs_for_download)}\")\n",
    "        logger.info(f\"Successfully downloaded: {successful_downloads}\")\n",
    "        logger.info(f\"Failed downloads: {failed_downloads}\")\n",
    "        logger.info(\"-----------------------------------\\n\")\n",
    "\n",
    "        for i, item in enumerate(results.get(\"data\", [])[:limit]):\n",
    "            if item.get(\"externalIds\") and item[\"externalIds\"].get(\"DOI\"):\n",
    "                doi = item[\"externalIds\"][\"DOI\"]\n",
    "                file_path = downloaded_files[i]  # Get corresponding result\n",
    "                if file_path:\n",
    "                    print(f\"      Downloaded DOI: {doi} PDF to: {file_path}\")\n",
    "                else:\n",
    "                    print(f\"      Failed to download PDF for DOI: {doi}\")\n",
    "    return downloaded_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Search Results (Top 3):\n",
      "      Downloading DOI 10.3390/math13030442\n",
      "      Downloading DOI 10.30574/ijsra.2024.11.1.0292\n",
      "      Downloading DOI 10.30574/wjaets.2024.11.1.0054\n",
      "\n",
      "      Downloaded DOI: 10.3390/math13030442 PDF to: downloads\\Sustainability__Accuracy__Fairness__and_Explainability__SAFE__Machine_Learning_in_Quantitative_Trading_10.3390_math13030442.pdf\n",
      "      Downloaded DOI: 10.30574/ijsra.2024.11.1.0292 PDF to: downloads\\Machine_learning_in_financial_markets__A_critical_review_of_algorithmic_trading_and_risk_management_10.30574_ijsra.2024.11.1.0292.pdf\n",
      "      Downloaded DOI: 10.30574/wjaets.2024.11.1.0054 PDF to: downloads\\Algorithmic_Trading_and_AI__A_Review_of_Strategies_and_Market_Impact_10.30574_wjaets.2024.11.1.0054.pdf\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    dois_titles = [\n",
    "        (\n",
    "            \"10.3390/math13030442\",\n",
    "            \"Sustainability, Accuracy, Fairness, and Explainability (SAFE) Machine Learning in Quantitative Trading\",\n",
    "        ),\n",
    "        (\n",
    "            \"10.30574/ijsra.2024.11.1.0292\",\n",
    "            \"Machine learning in financial markets: A critical review of algorithmic trading and risk management\",\n",
    "        ),\n",
    "        (\"10.30574/wjaets.2024.11.1.0054\", \"Algorithmic Trading and AI: A Review of Strategies and Market Impact\"),\n",
    "    ]\n",
    "\n",
    "    # Convert the list of tuples into the desired dictionary structure\n",
    "    results_data = []\n",
    "    for doi, title in dois_titles:\n",
    "        results_data.append({\"externalIds\": {\"DOI\": doi}, \"title\": title})\n",
    "\n",
    "    # Now, example_results_data is in the format that resembles results.get(\"data\", [])\n",
    "\n",
    "    # To use it with your `display_results_and_download` function, you would structure your results like this:\n",
    "    results_object = {\"data\": results_data}\n",
    "\n",
    "    file_path = await download_papers_parallel(results_object, limit=len(results_data), download=True)\n",
    "\n",
    "    if file_path:\n",
    "        logger.info(f\"PDF downloaded: {file_path}\")\n",
    "    else:\n",
    "        logger.error(\"Failed to download PDF.\")\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "\n",
    "\n",
    "async def hit_rate_limit_no_key():\n",
    "    # ... (your coroutine code) ...\n",
    "    paper_id = \"0f8f25af11d027bb6602639a4dc345c67af996c0\"  # Replace with a paper ID\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=title\"\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        i = 0\n",
    "        while True:\n",
    "            i += 1\n",
    "            try:\n",
    "                response = await client.get(url)\n",
    "                print(f\"Request: {i}, Status Code: {response.status_code}\")\n",
    "                response.raise_for_status()  # Raise HTTPStatusError for bad responses (4xx or 5xx)\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                print(f\"Request: {i}, Status Code: {e.response.status_code}\", flush=True)\n",
    "                if e.response.status_code == 429:\n",
    "                    print(\"Rate limit exceeded!\", flush=True)\n",
    "\n",
    "                    # --- Print ALL headers ---\n",
    "                    print(\"Response Headers:\")\n",
    "                    for name, value in e.response.headers.items():\n",
    "                        print(f\"  {name}: {value}\")\n",
    "\n",
    "                    retry_after = e.response.headers.get(\"Retry-After\")\n",
    "                    if retry_after:\n",
    "                        print(f\"Retry-After: {retry_after} seconds\", flush=True)\n",
    "                    break  # Exit the loop\n",
    "            except httpx.RequestError as e:\n",
    "                print(f\"Request {i} failed: {e}\", flush=True)\n",
    "                break  # Exit if network error\n",
    "            await asyncio.sleep(0.1)\n",
    "\n",
    "\n",
    "# --- In your Jupyter notebook or interactive session ---\n",
    "loop = asyncio.get_running_loop()  # Get the *existing* loop\n",
    "task = loop.create_task(hit_rate_limit_no_key())  # Or asyncio.create_task if you have python 3.7+\n",
    "# You can optionally await the task if you need to wait for it to finish:\n",
    "# await task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
