{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import httpx\n",
    "from parsel import Selector\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    # format=\"%(asctime)s | %(levelname)s | %(filename)s:%(lineno)d | %(funcName)s | %(message)s\",\n",
    "    # level=logging.DEBUG,\n",
    "    stream=sys.stdout,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_pdf_link(paper_url: str, doi: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extracts a direct PDF link by scraping the final article webpage.\n",
    "\n",
    "    Args:\n",
    "        paper_url: The initial article URL (could be a DOI link).\n",
    "\n",
    "    Returns:\n",
    "        The direct PDF URL if found, otherwise None.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Referer\": \"https://scholar.google.com\",  # Some sites require a referrer\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get final redirected URL (important for DOI links)\n",
    "        async with httpx.AsyncClient(timeout=20, follow_redirects=True) as client:\n",
    "            response = await client.get(paper_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            logger.info(f\"Final URL after redirect: {response.url}\")\n",
    "\n",
    "        final_url = str(response.url)\n",
    "\n",
    "        # async with httpx.AsyncClient(timeout=20, follow_redirects=True) as client:\n",
    "        #     response = await client.get(final_url, headers=headers)\n",
    "        #     response.raise_for_status()\n",
    "\n",
    "        selector = Selector(text=response.text)\n",
    "        pdf_links = selector.css(\"a::attr(href)\").getall()\n",
    "\n",
    "        doi_last_3 = doi[-3:] if len(doi) >= 3 else doi\n",
    "\n",
    "        PDF_PATTERNS = [\n",
    "            \".pdf\",  # Standard PDFs\n",
    "            \"/pdf\",  # PDF paths without extensions\n",
    "            \"pdf/\",  # PDF paths without extensions\n",
    "            \"download\",  # Download URLs\n",
    "            \"fulltext\",  # Full text links\n",
    "            \"article\",  # Article-specific PDF links\n",
    "            \"viewer\",  # Embedded viewer PDFs\n",
    "            \"content/pdf\",  # Used by Springer, Wiley, etc.\n",
    "        ]\n",
    "\n",
    "        for link in pdf_links:\n",
    "            if any(pattern in link.lower() for pattern in PDF_PATTERNS) and doi_last_3 in link.lower():\n",
    "                full_pdf_link = httpx.URL(final_url).join(link)\n",
    "                logger.info(f\"Found PDF link: {full_pdf_link}\")\n",
    "                return str(full_pdf_link)\n",
    "\n",
    "        logger.warning(\"No PDF link found on the page.\")\n",
    "        return None\n",
    "\n",
    "    except httpx.RequestError as e:\n",
    "        logger.error(f\"Error fetching page {paper_url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_paper(doi: str, title: str, output_dir: str = \"downloads\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Downloads a paper PDF given its DOI and title.\n",
    "    If Unpaywall fails, it scrapes the article page to find the PDF.\n",
    "\n",
    "    Args:\n",
    "        doi: The DOI of the paper.\n",
    "        title: The title of the paper (for the filename).\n",
    "        output_dir: The directory to save the downloaded PDF.\n",
    "\n",
    "    Returns:\n",
    "        The file path of the downloaded PDF if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Sanitize title for a safe filename\n",
    "        # safe_title = \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in title)\n",
    "        safe_title = re.sub(r\"[^\\w\\-_\\.]\", \"_\", title)\n",
    "        file_name = f\"{safe_title}_{doi.replace('/', '_')}.pdf\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        # Check if file already exists\n",
    "        if os.path.exists(f\"{file_path}\"):\n",
    "            logger.info(f\"Skipping download. PDF for DOI: {doi} already exists at {file_path}.\")\n",
    "            return file_path\n",
    "\n",
    "        pdf_url = f\"https://doi.org/{doi}\"\n",
    "        pdf_link = await scrape_pdf_link(pdf_url, doi)\n",
    "\n",
    "        if not pdf_link:\n",
    "            logger.error(f\"Could not find a PDF link for DOI: {doi}\")\n",
    "            return None\n",
    "\n",
    "        # Download the PDF\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            try:\n",
    "                headers = {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n",
    "                    \"Accept\": \"application/pdf,text/html,*/*\",\n",
    "                    \"Referer\": \"https://scholar.google.com\",  # Some sites require a referrer\n",
    "                }\n",
    "\n",
    "                response = await client.get(pdf_link, headers=headers, follow_redirects=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "                logger.info(f\"Content-Type received: {content_type}\")\n",
    "\n",
    "                if \"pdf\" not in content_type.lower():\n",
    "                    logger.error(\"The downloaded file is not a PDF!\")\n",
    "                    return None\n",
    "\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "\n",
    "                logger.info(f\"Downloaded PDF for DOI: {doi} to {file_path}\")\n",
    "                return file_path\n",
    "\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                logger.error(f\"HTTP error downloading PDF for DOI {doi}: {e.response.status_code}\")\n",
    "                return None\n",
    "            except httpx.RequestError as e:\n",
    "                logger.error(f\"Request error downloading PDF for DOI {doi}: {e}\")\n",
    "                return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"General error downloading PDF for DOI {doi}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_papers_parallel(results, limit=5, download=False, output_dir=\"downloads\"):\n",
    "    \"\"\"\n",
    "    Displays search results and optionally downloads PDFs in parallel.\n",
    "\n",
    "    Args:\n",
    "        results: The search results dictionary.\n",
    "        limit: The number of top results to display.\n",
    "        download: If True, download PDFs for the displayed results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Search Results (Top {limit}):\")\n",
    "    doi_title_pairs_for_download = []\n",
    "\n",
    "    for item in results.get(\"data\", [])[:limit]:\n",
    "        if download:\n",
    "            if item.get(\"externalIds\") and item[\"externalIds\"].get(\"DOI\"):\n",
    "                doi = item[\"externalIds\"][\"DOI\"]\n",
    "                title = item.get(\"title\", \"Unknown_Title\")\n",
    "                doi_title_pairs_for_download.append((doi, title))  # Collect DOI and title for parallel download\n",
    "                print(f\"      Downloading DOI {doi}\")  # Indicate download is initiated\n",
    "\n",
    "    print()\n",
    "\n",
    "    downloaded_files = []\n",
    "    if download and doi_title_pairs_for_download:\n",
    "        download_tasks = [download_paper(doi, title, output_dir) for doi, title in doi_title_pairs_for_download]\n",
    "        downloaded_files = await asyncio.gather(*download_tasks)\n",
    "\n",
    "        successful_downloads = 0\n",
    "        failed_downloads = 0\n",
    "        for file_path in downloaded_files:\n",
    "            if file_path:\n",
    "                successful_downloads += 1\n",
    "            else:\n",
    "                failed_downloads += 1\n",
    "\n",
    "        logger.info(\"--- Parallel Download Statistics ---\")\n",
    "        logger.info(f\"Total papers attempted: {len(doi_title_pairs_for_download)}\")\n",
    "        logger.info(f\"Successfully downloaded: {successful_downloads}\")\n",
    "        logger.info(f\"Failed downloads: {failed_downloads}\")\n",
    "        logger.info(\"-----------------------------------\\n\")\n",
    "\n",
    "        for i, item in enumerate(results.get(\"data\", [])[:limit]):\n",
    "            if item.get(\"externalIds\") and item[\"externalIds\"].get(\"DOI\"):\n",
    "                doi = item[\"externalIds\"][\"DOI\"]\n",
    "                file_path = downloaded_files[i]  # Get corresponding result\n",
    "                if file_path:\n",
    "                    print(f\"      Downloaded DOI: {doi} PDF to: {file_path}\")\n",
    "                else:\n",
    "                    print(f\"      Failed to download PDF for DOI: {doi}\")\n",
    "    return downloaded_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Search Results (Top 3):\n",
      "      Downloading DOI 10.3390/math13030442\n",
      "      Downloading DOI 10.30574/ijsra.2024.11.1.0292\n",
      "      Downloading DOI 10.30574/wjaets.2024.11.1.0054\n",
      "\n",
      "2025-02-13 18:59:03,332 | INFO | 3446133886.py:25 | download_paper | Skipping download. PDF for DOI: 10.3390/math13030442 already exists at downloads\\Sustainability__Accuracy__Fairness__and_Explainability__SAFE__Machine_Learning_in_Quantitative_Trading_10.3390_math13030442.pdf.\n",
      "2025-02-13 18:59:03,333 | INFO | 3446133886.py:25 | download_paper | Skipping download. PDF for DOI: 10.30574/ijsra.2024.11.1.0292 already exists at downloads\\Machine_learning_in_financial_markets__A_critical_review_of_algorithmic_trading_and_risk_management_10.30574_ijsra.2024.11.1.0292.pdf.\n",
      "2025-02-13 18:59:03,333 | INFO | 3446133886.py:25 | download_paper | Skipping download. PDF for DOI: 10.30574/wjaets.2024.11.1.0054 already exists at downloads\\Algorithmic_Trading_and_AI__A_Review_of_Strategies_and_Market_Impact_10.30574_wjaets.2024.11.1.0054.pdf.\n",
      "2025-02-13 18:59:03,334 | INFO | 791324778.py:36 | download_papers_parallel | --- Parallel Download Statistics ---\n",
      "2025-02-13 18:59:03,334 | INFO | 791324778.py:37 | download_papers_parallel | Total papers attempted: 3\n",
      "2025-02-13 18:59:03,335 | INFO | 791324778.py:38 | download_papers_parallel | Successfully downloaded: 3\n",
      "2025-02-13 18:59:03,335 | INFO | 791324778.py:39 | download_papers_parallel | Failed downloads: 0\n",
      "2025-02-13 18:59:03,336 | INFO | 791324778.py:40 | download_papers_parallel | -----------------------------------\n",
      "\n",
      "      Downloaded DOI: 10.3390/math13030442 PDF to: downloads\\Sustainability__Accuracy__Fairness__and_Explainability__SAFE__Machine_Learning_in_Quantitative_Trading_10.3390_math13030442.pdf\n",
      "      Downloaded DOI: 10.30574/ijsra.2024.11.1.0292 PDF to: downloads\\Machine_learning_in_financial_markets__A_critical_review_of_algorithmic_trading_and_risk_management_10.30574_ijsra.2024.11.1.0292.pdf\n",
      "      Downloaded DOI: 10.30574/wjaets.2024.11.1.0054 PDF to: downloads\\Algorithmic_Trading_and_AI__A_Review_of_Strategies_and_Market_Impact_10.30574_wjaets.2024.11.1.0054.pdf\n",
      "2025-02-13 18:59:03,336 | INFO | 1146728998.py:27 | main | PDF downloaded: ['downloads\\\\Sustainability__Accuracy__Fairness__and_Explainability__SAFE__Machine_Learning_in_Quantitative_Trading_10.3390_math13030442.pdf', 'downloads\\\\Machine_learning_in_financial_markets__A_critical_review_of_algorithmic_trading_and_risk_management_10.30574_ijsra.2024.11.1.0292.pdf', 'downloads\\\\Algorithmic_Trading_and_AI__A_Review_of_Strategies_and_Market_Impact_10.30574_wjaets.2024.11.1.0054.pdf']\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    dois_titles = [\n",
    "        (\n",
    "            \"10.3390/math13030442\",\n",
    "            \"Sustainability, Accuracy, Fairness, and Explainability (SAFE) Machine Learning in Quantitative Trading\",\n",
    "        ),\n",
    "        (\n",
    "            \"10.30574/ijsra.2024.11.1.0292\",\n",
    "            \"Machine learning in financial markets: A critical review of algorithmic trading and risk management\",\n",
    "        ),\n",
    "        (\"10.30574/wjaets.2024.11.1.0054\", \"Algorithmic Trading and AI: A Review of Strategies and Market Impact\"),\n",
    "    ]\n",
    "\n",
    "    # Convert the list of tuples into the desired dictionary structure\n",
    "    results_data = []\n",
    "    for doi, title in dois_titles:\n",
    "        results_data.append({\"externalIds\": {\"DOI\": doi}, \"title\": title})\n",
    "\n",
    "    # Now, example_results_data is in the format that resembles results.get(\"data\", [])\n",
    "\n",
    "    # To use it with your `display_results_and_download` function, you would structure your results like this:\n",
    "    results_object = {\"data\": results_data}\n",
    "\n",
    "    file_path = await download_papers_parallel(results_object, limit=len(results_data), download=True)\n",
    "\n",
    "    if file_path:\n",
    "        logger.info(f\"PDF downloaded: {file_path}\")\n",
    "    else:\n",
    "        logger.error(\"Failed to download PDF.\")\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request: 1, Status Code: 200\n",
      "Request: 2, Status Code: 200\n",
      "Request: 3, Status Code: 200\n",
      "Request: 4, Status Code: 200\n",
      "Request: 5, Status Code: 429\n",
      "Request: 5, Status Code: 429\n",
      "Rate limit exceeded!\n",
      "Response Headers:\n",
      "  content-type: application/json\n",
      "  content-length: 174\n",
      "  connection: keep-alive\n",
      "  date: Fri, 14 Feb 2025 16:31:23 GMT\n",
      "  x-amz-apigw-id: F-5qaHezvHcEdwg=\n",
      "  x-amzn-requestid: 55f58e7c-a53b-4f0d-9bcc-f06dfa089e16\n",
      "  x-amzn-errortype: TooManyRequestsException\n",
      "  x-cache: Error from cloudfront\n",
      "  via: 1.1 9a9e1d634ed04ebc88e3edf6c14456fe.cloudfront.net (CloudFront)\n",
      "  x-amz-cf-pop: JNB50-C1\n",
      "  x-amz-cf-id: 0SeBs6fKZDyROtgzLIRo3hDQT79c3loDWHIMWXwCkaWxTsSES-xw0w==\n"
     ]
    }
   ],
   "source": [
    "async def hit_rate_limit_no_key():\n",
    "    # ... (your coroutine code) ...\n",
    "    paper_id = \"0f8f25af11d027bb6602639a4dc345c67af996c0\"  # Replace with a paper ID\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=title\"\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        i = 0\n",
    "        while True:\n",
    "            i += 1\n",
    "            try:\n",
    "                response = await client.get(url)\n",
    "                print(f\"Request: {i}, Status Code: {response.status_code}\")\n",
    "                response.raise_for_status()  # Raise HTTPStatusError for bad responses (4xx or 5xx)\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                print(f\"Request: {i}, Status Code: {e.response.status_code}\", flush=True)\n",
    "                if e.response.status_code == 429:\n",
    "                    print(\"Rate limit exceeded!\", flush=True)\n",
    "\n",
    "                    # --- Print ALL headers ---\n",
    "                    print(\"Response Headers:\")\n",
    "                    for name, value in e.response.headers.items():\n",
    "                        print(f\"  {name}: {value}\")\n",
    "\n",
    "                    retry_after = e.response.headers.get(\"Retry-After\")\n",
    "                    if retry_after:\n",
    "                        print(f\"Retry-After: {retry_after} seconds\", flush=True)\n",
    "                    break  # Exit the loop\n",
    "            except httpx.RequestError as e:\n",
    "                print(f\"Request {i} failed: {e}\", flush=True)\n",
    "                break  # Exit if network error\n",
    "            await asyncio.sleep(0.1)\n",
    "\n",
    "\n",
    "# --- In your Jupyter notebook or interactive session ---\n",
    "loop = asyncio.get_running_loop()  # Get the *existing* loop\n",
    "task = loop.create_task(hit_rate_limit_no_key())  # Or asyncio.create_task if you have python 3.7+\n",
    "# You can optionally await the task if you need to wait for it to finish:\n",
    "# await task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
